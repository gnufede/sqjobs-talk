#+TITLE: SQJobs
#+AUTHOR: Federico Mon
#+EMAIL: federico.mon@ticketea.com
#+DATE: 2015-11-21
#+OPTIONS: num:nil toc:nil todo:nil
#+REVEAL_ROOT: ./reveal.js/
# #+REVEAL_ROOT: https://cdnjs.cloudflare.com/ajax/libs/reveal.js/3.2.0/
#+REVEAL_EXTRA_CSS: ./custom.css
#+REVEAL_SLIDE_NUMBER: nil
#+REVEAL_THEME: league
#+REVEAL_BACKGROUND: #272822


* Intro
** Intro to Async Queued Task Execution
#+NAME:   fig:intro
[[./img/intro.png]]

** SQJobs implementation
#+NAME:   fig:sqjobs
[[./img/sqjobs1.png]]
# ** Sqjobs is a simple queue jobs system.
# ** Jobs are run asynchronously "in background"
# ** Jobs can be run even in other machines
# ** Because a Queue of jobs is used
# ** Several ends can create a Job
# ** Worker is the end where the job is consumed
# ** The worker is implemented in Python.
# ** The broker can be Python or other.
# ** At the moment, the broker is SQS.
# ** Cheatsheet

* History
** Not so long ago, Celery was used in Ticketea.
#+NAME:   fig:celery
[[./img/celery.gif]]
#+BEGIN_NOTES
  * Usabamos Celery con rabbitmq en modelo activo-activo.
  * Dos servidores de rabbitmq que en principio funcionan como uno, pero debería
    ser más tolerante a fallos.
#+END_NOTES
** Some Split-brains happened in RabbitMQ, leading to lot of effort debugging the situation.
# #+NAME:   fig:splitbrain
# [[./img/splitbrain.gif]]
#+NAME:   fig:brain
[[./img/brain.gif]]
*** 
#+NAME:   fig:issue
[[./img/maraujop1.png]]
*** 
#+NAME:   fig:issue2
[[./img/maraujop2.png]]

** Migrations to OpenERP were taking >48 hours
#+NAME:   fig:sleepy
[[./img/sleepy.gif]]

** The goal was to quit using Celery and RabbitMQ and use something simpler instead
#+NAME:   fig:rainbow
[[./img/rainbow.gif]]


** For more details you can ask Iñaki or Miguel
~@igalarzab~ and ~@maraujop~

* The problem
** Resilience in a network
"Resiliency is the ability to provide and maintain an acceptable
level of service in the face of faults and challenges to normal operation."
#+NAME:   fig:outage
[[./img/outage.gif]]

** Possible solutions:
*** Use an existing solution
 RQ (Redis)
#+NAME:   fig:confused
[[./img/confused.gif]]
*** Create our own solution
#+NAME:   fig:ourselves
[[./img/ourselves.gif]]
#+ATTR_REVEAL: :frag appear
 * Easier to debug
 * Choose the broker we want
 * Scratch your own itch


* Crafting the solution
** Different delivery guarantees
#+ATTR_REVEAL: :frag appear
 * Only-once-delivery
 * At-least-once-delivery
** If we pick only-once-delivery...
*** What if messages are dropped?
# #+NAME:   fig:FAIL
# [[./img/fail.gif]]
#+NAME:   fig:messages_dropped
[[./img/messages_dropped.gif]]

** If we pick at-least-once-delivery...
*** Then, what if messages are received more than once?
#+NAME:   fig:double_fail
[[./img/double_fail.gif]]
*** Write Idempotent Jobs
#+NAME:   fig:Idempotence
[[./img/idem.gif]]

* The success
** Migration to OpenERP took 6 hours
#+NAME:   fig:yes
[[./img/yes.gif]]
** SQjobs workers are quite fast, and easier to debug than Celery's
At least for us :)
#+NAME:   fig:MELOCOTONAZO
[[./img/melocotonazo.gif]]

* The Job
** Must be in ~jobs.py~
** ~models.py~ must exist
** Sample Job
#+BEGIN_SRC python
from sqjobs.job import Job

class AdderJob(Job):
    name = 'adder_job'
    queue = 'my_queue'
 
    def run(self, *args, **kwargs):
        return sum(args)
#+END_SRC

* Launching a Job
** From python
#+BEGIN_SRC python
from sqjobs import create_sqs_broker
from myapp.jobs import AdderJob

kwargs = {
    'access_key': settings.SQJOBS_SQS_ACCESS_KEY,
    'secret_key': settings.SQJOBS_SQS_ACCESS_KEY
}
broker = create_sqs_broker(**kwargs)
broker.add_job(AdderJob, *[1, 2, 3, 4])
#+END_SRC

** From PHP
#+BEGIN_SRC php
$payload = array(
    'name' => $task_name,
    'args' => $args,
    'kwargs' => $kwargs
);
$json_payload = json_encode($payload);

$this->_sqs = new AmazonSQS($amazon_config['aws_key'], $amazon_config['aws_secret_key']);
$result = $this->_sqs->send_message($this->_queue_urls[$queue_name], base64_encode($json_payload));
#+END_SRC

* Eager mode
** Eager mode is a simpler execution mode
** Tasks are run synchronously
** By the broker itself
** So there is no need for a queue nor running workers.
** Meant for development and unit testing.
** Sample execution
#+BEGIN_SRC python
>>> from sqjobs import create_eager_broker
>>> broker = create_eager_broker()
>>> from jobs import AdderJob
>>> job_added = broker.add_job(AdderJob, *[1, 2, 3])
>>> job_added 
('fdb005d3-276f-4f75-8e8e-c8fcde67043c', AdderJob())
>>> job_added[1].result
6
#+END_SRC

* The Worker
** Workers listen in a queue and execute jobs
** Built as a django command
** You can launch as many as you want
** Usage:
#+BEGIN_SRC bash
$ ./manage.py sqjobs worker $queue_name
#+END_SRC

* A Result-backed Job
** Status of the job is stored in a database.
** It uses a Django model, Django is needed here.
** Can be used, for example, in a web application, to know when the job is done or fails, and act accordingly.
** Sample ResultJob
#+BEGIN_SRC python
from sqjobs.contrib.django.djsqjobs.result_job import ResultJob

class DummyResultJob(ResultJob):
    name = 'dummy_result_job'
    queue = 'dummy_queue'
 
    def run(self, *args, **kwargs):
        pass
#+END_SRC
** How to use a resultjob
#+BEGIN_SRC python
>>> from sqjobs.contrib.django.djsqjobs.models import JobStatus
>>> my_job = JobStatus.objects.get(job_id='1234')
>>> if my_job.status == JobStatus.SUCCESS:
...     print my_job.result
#+END_SRC

* A Periodic task
** Will be executed like if they were in a crontab.
** This requires another piece of software
called ~Beater~
** Cron ranges can be localized to a timezone
** And support daylight saving changes.
** Sample PeriodicJob
#+BEGIN_SRC python
from djsqjobs import PeriodicJob

class DummyPeriodicJob(PeriodicJob):
    name = "dummy_periodic_job"
    queue = "my_queue"

    schedule = "1 0 * * *"
    timezone = "Europe/Madrid"

    def run(self, *args, **kwargs):
        pass
#+END_SRC

* The Beater
** A special component that queue jobs at the right moment.
** By waking up every certain time, check what jobs should be queued, and reprogram them.
** You can launch as many as you need.
** So if any of them dies, the others will queue your job.
** They use the django database to synchronise and launch the job only once.
** Despite your jobs should be idempotent.
** Usage:
#+BEGIN_SRC bash
$ ./manage.py sqjobs beater $queue_name
#+END_SRC

* Set up and Tear down
** Job execution is divided in three different stages:
~set_up~, ~run~, ~tear_down~
#+ATTR_REVEAL: :frag appear
 * Only ~run~ is mandatory
 * ~set_up~ would be called before run if exists
 * And ~tear_down~ right after ~run~ if exists.

** Sample Job
#+BEGIN_SRC python
from abc import abstractmethod, ABCMeta
from six import add_metaclass
import logging

logger = logging.getLogger('timed_job')

@add_metaclass(ABCMeta)
class TimedJob(Job):

    def set_up(self, *args, **kwargs):
        super(TimedJob, self).set_up(*args, **kwargs)
        self.start_time = datetime.now()

    def tear_down(self, *args, **kwargs):
        end_time = datetime.now()
        delta = end_time - self.start_time
        logger.info('%s finished in %d seconds' % (self.name, (delta * 1000).seconds))
        super(TimedJob, self).tear_down(*args, **kwargs)

    @abstractmethod
    def run(self, *args, **kwargs):
        raise NotImplementedError
#+END_SRC

* Failure and Success
** We can define failure and success methods
 ~on_success~ and ~on_failure~ methods will be called
depending on the output of our job execution.

** Example of ~on_success~ and ~on_failure~
#+BEGIN_SRC python
from abc import abstractmethod, ABCMeta
from six import add_metaclass
import logging

logger = logging.getLogger('logger_job')

@add_metaclass(ABCMeta)
class LoggerJob(Job):

    def on_success(self, *args, **kwargs):
        logger.log('Successfully finished job %s' % self.name)
        super(LoggerJob, self).on_success(*args, **kwargs)

    def on_failure(self, *args, **kwargs):
        logger.log('Failed job %s' % self.name)
        super(LoggerJob, self).on_failure(*args, **kwargs)

    @abstractmethod
    def run(self, *args, **kwargs):
        raise NotImplementedError
#+END_SRC
